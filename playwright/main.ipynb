{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8120ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# Set up basic logging for better feedback\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ef2fc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrap():\n",
    "    \"\"\"\n",
    "    Launches a browser, navigates to billygrados.com, and scrapes blog post data.\n",
    "    \"\"\"\n",
    "    posts_data = []\n",
    "    BASE_URL = \"https://billygrados.com\"\n",
    "    \n",
    "    logging.info(\"Starting the scraping process...\")\n",
    "    p = await async_playwright().start()\n",
    "    browser = await p.chromium.launch(headless=False) # Set headless=False to watch the browser\n",
    "    page = await browser.new_page()\n",
    "    \n",
    "    try:\n",
    "        logging.info(f\"Navigating to {BASE_URL}...\")\n",
    "        await page.goto(BASE_URL, wait_until=\"domcontentloaded\")\n",
    "        logging.info(\"Page loaded successfully.\")\n",
    "\n",
    "        # Wait for the post feed container to be visible to ensure content is loaded\n",
    "        await page.locator('div.post-feed').wait_for(timeout=10000)\n",
    "\n",
    "        # Locate all the article elements that represent a blog post\n",
    "        post_cards = await page.locator('article.post-card').all()\n",
    "        logging.info(f\"Found {len(post_cards)} posts on the page.\")\n",
    "\n",
    "        for post in post_cards:\n",
    "            # Use locators to find elements within each post card\n",
    "            title_element = post.locator('h2.post-card-title')\n",
    "            link_element = post.locator('a.post-card-image-link')\n",
    "            excerpt_element = post.locator('div.post-card-excerpt')\n",
    "\n",
    "            # Extract the data\n",
    "            title = await title_element.inner_text()\n",
    "            relative_url = await link_element.get_attribute('href')\n",
    "            excerpt = await excerpt_element.inner_text()\n",
    "\n",
    "            # Clean up and format the data\n",
    "            url = f\"{BASE_URL}{relative_url}\" if relative_url else \"No URL Found\"\n",
    "            \n",
    "            posts_data.append({\n",
    "                \"title\": title.strip(),\n",
    "                \"url\": url,\n",
    "                \"excerpt\": excerpt.strip()\n",
    "            })\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred during scraping: {e}\")\n",
    "    finally:\n",
    "        logging.info(\"Closing the browser.\")\n",
    "        await browser.close()\n",
    "            \n",
    "    logging.info(f\"Scraping finished. Extracted {len(posts_data)} posts.\")\n",
    "    return posts_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f02292f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-17 19:41:58,780 - INFO - Starting the scraping process...\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# In a Jupyter Notebook or IPython environment, you can run the async function directly\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m scraped_data = \u001b[38;5;28;01mawait\u001b[39;00m scrap()\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# If you were running this as a .py script, you would use:\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# scraped_data = asyncio.run(scrape_billygrados())\u001b[39;00m\n\u001b[32m      6\u001b[39m \n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Now, let's use pandas to display the data neatly\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m scraped_data:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mscrap\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      6\u001b[39m BASE_URL = \u001b[33m\"\u001b[39m\u001b[33mhttps://billygrados.com\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m logging.info(\u001b[33m\"\u001b[39m\u001b[33mStarting the scraping process...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m p = \u001b[38;5;28;01mawait\u001b[39;00m async_playwright().start()\n\u001b[32m     10\u001b[39m browser = \u001b[38;5;28;01mawait\u001b[39;00m p.chromium.launch(headless=\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;66;03m# Set headless=False to watch the browser\u001b[39;00m\n\u001b[32m     11\u001b[39m page = \u001b[38;5;28;01mawait\u001b[39;00m browser.new_page()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\_workspaces\\live\\playwright\\.venv\\Lib\\site-packages\\playwright\\async_api\\_context_manager.py:51\u001b[39m, in \u001b[36mPlaywrightContextManager.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> AsyncPlaywright:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.\u001b[34m__aenter__\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\_workspaces\\live\\playwright\\.venv\\Lib\\site-packages\\playwright\\async_api\\_context_manager.py:46\u001b[39m, in \u001b[36mPlaywrightContextManager.__aenter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m playwright_future.done():\n\u001b[32m     45\u001b[39m     playwright_future.cancel()\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m playwright = AsyncPlaywright(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     47\u001b[39m playwright.stop = \u001b[38;5;28mself\u001b[39m.\u001b[34m__aexit__\u001b[39m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m playwright\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\_workspaces\\live\\playwright\\.venv\\Lib\\site-packages\\playwright\\_impl\\_transport.py:120\u001b[39m, in \u001b[36mPipeTransport.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    117\u001b[39m         startupinfo.wShowWindow = subprocess.SW_HIDE\n\u001b[32m    119\u001b[39m     executable_path, entrypoint_path = compute_driver_executable()\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     \u001b[38;5;28mself\u001b[39m._proc = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_subprocess_exec(\n\u001b[32m    121\u001b[39m         executable_path,\n\u001b[32m    122\u001b[39m         entrypoint_path,\n\u001b[32m    123\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrun-driver\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    124\u001b[39m         stdin=asyncio.subprocess.PIPE,\n\u001b[32m    125\u001b[39m         stdout=asyncio.subprocess.PIPE,\n\u001b[32m    126\u001b[39m         stderr=_get_stderr_fileno(),\n\u001b[32m    127\u001b[39m         limit=\u001b[32m32768\u001b[39m,\n\u001b[32m    128\u001b[39m         env=env,\n\u001b[32m    129\u001b[39m         startupinfo=startupinfo,\n\u001b[32m    130\u001b[39m     )\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    132\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_error_future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Program Files\\Python\\Python313\\Lib\\asyncio\\subprocess.py:224\u001b[39m, in \u001b[36mcreate_subprocess_exec\u001b[39m\u001b[34m(program, stdin, stdout, stderr, limit, *args, **kwds)\u001b[39m\n\u001b[32m    221\u001b[39m loop = events.get_running_loop()\n\u001b[32m    222\u001b[39m protocol_factory = \u001b[38;5;28;01mlambda\u001b[39;00m: SubprocessStreamProtocol(limit=limit,\n\u001b[32m    223\u001b[39m                                                     loop=loop)\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m transport, protocol = \u001b[38;5;28;01mawait\u001b[39;00m loop.subprocess_exec(\n\u001b[32m    225\u001b[39m     protocol_factory,\n\u001b[32m    226\u001b[39m     program, *args,\n\u001b[32m    227\u001b[39m     stdin=stdin, stdout=stdout,\n\u001b[32m    228\u001b[39m     stderr=stderr, **kwds)\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Process(transport, protocol, loop)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Program Files\\Python\\Python313\\Lib\\asyncio\\base_events.py:1802\u001b[39m, in \u001b[36mBaseEventLoop.subprocess_exec\u001b[39m\u001b[34m(self, protocol_factory, program, stdin, stdout, stderr, universal_newlines, shell, bufsize, encoding, errors, text, *args, **kwargs)\u001b[39m\n\u001b[32m   1800\u001b[39m     debug_log = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mexecute program \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprogram\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m   1801\u001b[39m     \u001b[38;5;28mself\u001b[39m._log_subprocess(debug_log, stdin, stdout, stderr)\n\u001b[32m-> \u001b[39m\u001b[32m1802\u001b[39m transport = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_subprocess_transport(\n\u001b[32m   1803\u001b[39m     protocol, popen_args, \u001b[38;5;28;01mFalse\u001b[39;00m, stdin, stdout, stderr,\n\u001b[32m   1804\u001b[39m     bufsize, **kwargs)\n\u001b[32m   1805\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._debug \u001b[38;5;129;01mand\u001b[39;00m debug_log \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1806\u001b[39m     logger.info(\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m'\u001b[39m, debug_log, transport)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Program Files\\Python\\Python313\\Lib\\asyncio\\base_events.py:539\u001b[39m, in \u001b[36mBaseEventLoop._make_subprocess_transport\u001b[39m\u001b[34m(self, protocol, args, shell, stdin, stdout, stderr, bufsize, extra, **kwargs)\u001b[39m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_make_subprocess_transport\u001b[39m(\u001b[38;5;28mself\u001b[39m, protocol, args, shell,\n\u001b[32m    536\u001b[39m                                      stdin, stdout, stderr, bufsize,\n\u001b[32m    537\u001b[39m                                      extra=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    538\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create subprocess transport.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[31mNotImplementedError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# In a Jupyter Notebook or IPython environment, you can run the async function directly\n",
    "scraped_data = await scrap()\n",
    "\n",
    "# If you were running this as a .py script, you would use:\n",
    "# scraped_data = asyncio.run(scrape_billygrados())\n",
    "\n",
    "# Now, let's use pandas to display the data neatly\n",
    "if scraped_data:\n",
    "    df = pd.DataFrame(scraped_data)\n",
    "    print(\"\\n--- Scraped Data ---\")\n",
    "    display(df)\n",
    "else:\n",
    "    print(\"No data was scraped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba3fddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if scraped_data:\n",
    "    try:\n",
    "        df.to_csv(\"billygrados_posts_playwright.csv\", index=False, encoding='utf-8')\n",
    "        logging.info(\"Data successfully saved to billygrados_posts_playwright.csv\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving data to CSV: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
